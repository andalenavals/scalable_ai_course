from tqdm import tqdm
import numpy as np
import torch
import scalable_ml.model_utilities as util


def train_model(ml_model, device, train_loader, val_loader, epochs):
    """
    Trains a pytorch model

    For the optimizer we use the Adam algorithm
    - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
    - https://arxiv.org/abs/1412.6980

   For the loss function we use the mean squared error (squared L2 norm) or DSSIM (derived from SSIM)
   - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html
   - https://torchmetrics.readthedocs.io/en/stable/image/structural_similarity.html

    :param ml_model:
    :type ml_model: torch Model object
    :param device: device on which training is performed
    :type device: basestring
    :param train_loader: training dataset
    :type train_loader: torch DataLoader
    :param val_loader:  training validation set
    :type train_loader: torch DataLoader
    :param epochs: number of epochs used for training
    :type epochs: int
    """
    params = [
        {'params': ml_model.parameters()}
    ]

    #optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-5)
    optimizer = torch.optim.AdamW(params, lr=1e-5, weight_decay=1e-8)
    loss_function = torch.nn.MSELoss()

    losses = {'train_loss': [], 'val_loss': []}

    # loop for training
    for epoch in range(epochs):
        train_loss = train_unsupervised_epoch(ml_model, device, train_loader, loss_function, optimizer)
        val_loss = val_unsupervised_epoch(ml_model, device, val_loader, loss_function)

        print(f'Epoch {epoch + 1}/{epochs} \t train loss {train_loss:8f}')
        print(f'Epoch {epoch + 1}/{epochs} \t val loss {val_loss:8f}')
        losses['train_loss'].append(train_loss)

        losses['val_loss'].append(val_loss)

    return losses


def train_unsupervised_epoch(ml_model, device, dataloader, loss_fn, optimizer):
    """
    Performs one training epoch.

    :param autoencoders: Model that is being trained
    :type autoencoders: list of torch Model object
    :param device: device on which training is performed
    :type device: basestring
    :param dataloader: training dataset
    :type dataloader: torch DataLoader
    :param loss_fn: loss-function
    :type loss_fn: loss-function object
    :param optimizer: model optimizer
    :type optimizer: torch optimizer object
    :param exercise: exercise sheet (1,2,3, ...)
    :type exercise: int
    :return: mean of training loss during epoch
    :rtype: float
    """

    train_loss = []

    # required since we shift model to eval model in accuracy function
    ml_model.train()  # sets model to train mode, i.e. activates dropout layers not used for inference

     # create progress bar
    loop = tqdm(dataloader)

    # iterate over batches generated by dataloader
    for ix, image_batch in enumerate(loop):
        # each batch consists of 128 images
        image_batch = image_batch.to(device)

        # Initializing a gradient as 0 so there is no mixing of gradient among the batches
        optimizer.zero_grad()

        # Forward pass: make prediction for this batch
        pred = ml_model(image_batch)

        # compute loss
        loss = loss_fn(pred, image_batch)

        # Backward propagation for computing the gradient of loss function
        loss.backward()

        # Update the weights and the biases of our current model
        optimizer.step()

        # update progress bar
        loop.set_postfix(loss=loss.detach().cpu().numpy())

        train_loss.append(loss.detach().cpu().numpy())

    return np.mean(train_loss)


def val_unsupervised_epoch(ml_model, device, val_loader, loss_fn):
    ml_model.eval()  # Set the model to evaluation mode, disabling dropout and using population
    # statistics for batch normalization.

    val_loss = []

    with torch.no_grad():
        for ix, val_image_batch in enumerate(val_loader):

            val_image_batch = val_image_batch.to(device)

            pred = ml_model(val_image_batch)

            # compute loss
            loss = loss_fn(pred, val_image_batch)
            val_loss.append(loss.detach().cpu().numpy())

    return np.mean(val_loss)
