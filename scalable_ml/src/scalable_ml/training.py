from tqdm import tqdm
import numpy as np
import torch
import scalable_ml.model_utilities as util


def train_model(ml_model, device, train_loader, val_loader, epochs, exercise=1):
    """
    Trains a pytorch model

    For the optimizer we use the Adam algorithm
    - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
    - https://arxiv.org/abs/1412.6980

   For the loss function we use the mean squared error (squared L2 norm) or DSSIM (derived from SSIM)
   - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html
   - https://torchmetrics.readthedocs.io/en/stable/image/structural_similarity.html

    :param ml_model:
    :type ml_model: torch Model object
    :param device: device on which training is performed
    :type device: basestring
    :param train_loader: training dataset
    :type train_loader: torch DataLoader
    :param epochs: number of epochs used for training
    :type epochs: int
    :param val_loader:  training validation set
    :type train_loader: torch DataLoader
    """
    params = [
        {'params': ml_model.parameters()}
    ]

    optimizer = torch.optim.Adam(params, lr=1e-4, weight_decay=1e-5)
    loss_function = torch.nn.CrossEntropyLoss()

    losses = {'train_loss': [], 'val_loss': []}
    accuracy = {'train_accuracy': [], 'val_accuracy': []}
    perf_metric = 'dice' if exercise == 2 else 'accuracy'

    # loop for training
    for epoch in range(epochs):
        train_loss, train_accuracy = train_epoch(ml_model, device, train_loader, loss_function, optimizer, exercise)
        val_loss, val_accuracy = val_epoch(ml_model, device, val_loader, loss_function, exercise)

        print(f'Epoch {epoch + 1}/{epochs} \t train loss {train_loss:8f} \t train {perf_metric} {train_accuracy:1f}')
        print(f'Epoch {epoch + 1}/{epochs} \t val loss {val_loss:8f} \t val {perf_metric} {val_accuracy:1f}')
        losses['train_loss'].append(train_loss)
        accuracy['train_accuracy'].append(train_accuracy)

        losses['val_loss'].append(val_loss)
        accuracy['val_accuracy'].append(val_accuracy)

    return losses, accuracy


def train_epoch(ml_model, device, dataloader, loss_fn, optimizer, exercise):
    """
    Performs one training epoch.

    :param ml_model: Model that is being trained
    :type ml_model: list of torch Model object
    :param device: device on which training is performed
    :type device: basestring
    :param dataloader: training dataset
    :type dataloader: torch DataLoader
    :param loss_fn: loss-function
    :type loss_fn: loss-function object
    :param optimizer: model optimizer
    :type optimizer: torch optimizer object
    :param exercise: exercise sheet (1,2,3, ...)
    :type exercise: int
    :return: mean of training loss during epoch
    :rtype: float
    """

    train_loss = []
    train_accuracy = []

    # required since we shift model to eval model in accuracy function
    ml_model.train()  # sets model to train mode, i.e. activates dropout layers not used for inference

    # create progress bar
    loop = tqdm(dataloader)

    # iterate over batches generated by dataloader
    for ix, (image_batch, target_batch) in enumerate(loop):
        # each batch consists of 128 images
        image_batch, target_batch = image_batch.to(device), target_batch.to(device)

        # Initializing a gradient as 0 so there is no mixing of gradient among the batches
        optimizer.zero_grad()

        # Forward pass: make prediction for this batch
        pred = ml_model(image_batch)

        # compute loss
        loss = loss_fn(pred, target_batch)

        # Backward propagation for computing the gradient of loss function
        loss.backward()

        # Update the weights and the biases of our current model
        optimizer.step()

        # update progress bar
        loop.set_postfix(loss=loss.detach().cpu().numpy())

        train_loss.append(loss.detach().cpu().numpy())

        # accuracy of the batch (for exercise 1, has to be modified for exercise 2)
        if exercise==1:
            is_correct = util.accuracy(image_batch, target_batch, ml_model)
        elif exercise==2:
            #is_correct=0
            is_correct = util.segmentation_accuracy(image_batch, target_batch, ml_model)
        train_accuracy.append(is_correct)

    return np.mean(train_loss), np.mean(train_accuracy)



def val_epoch(ml_model, device, val_loader, loss_fn, exercise):
    ml_model.eval()  # Set the model to evaluation mode, disabling dropout and using population
    # statistics for batch normalization.

    val_loss = []
    val_accuracy = []

    with torch.no_grad():
        for ix, (val_image_batch, val_target_batch) in enumerate(val_loader):

            val_image_batch, val_target_batch = val_image_batch.to(device), val_target_batch.to(device)

            pred = ml_model(val_image_batch)

            # compute loss
            loss = loss_fn(pred, val_target_batch)
            val_loss.append(loss.detach().cpu().numpy())

            # compute accuracy (exercise 1) or dice score (exercise 2)
            if exercise==1:
                is_correct = util.accuracy(val_image_batch, val_target_batch, ml_model)
            elif exercise==2:
                is_correct = util.segmentation_accuracy(val_image_batch, val_target_batch, ml_model)
            val_accuracy.append(is_correct)

    return np.mean(val_loss), np.mean(val_accuracy)
